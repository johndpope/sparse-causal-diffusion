# EditCtrl + SCD + HRR Phase 1: Local Context + HRR Text Enhancement
#
# Adds HRR (Holographic Reduced Representations) text enhancement to EditCtrl.
# HRR routes text tokens to semantic channels via frequency-domain operations,
# enabling richer conditioning and (optionally) cross-caption editing signal.
#
# HRR adds ~0.4 MB of trainable params (188K params at bf16).
# With cross-caption loss, peak VRAM increases ~2 GB.
#
# Prerequisites:
#   - Trained SCD LoRA checkpoint (from scd_finetune.yaml)
#   - Pre-encoded latents and cached embeddings
#
# Usage:
#   cd /home/johndpope/Documents/GitHub/ltx2-omnitransfer/packages/ltx-trainer
#   python -m ltx_trainer.train --config /home/johndpope/Documents/GitHub/sparse-causal-diffusion/configs/editctrl_scd_hrr_phase1.yaml

seed: 42
output_dir: /home/johndpope/Documents/GitHub/sparse-causal-diffusion/outputs/editctrl_scd_hrr_phase1

model:
  model_path: /media/2TB/ltx-models/ltx2/ltx-2-19b-dev.safetensors
  text_encoder_path: /media/2TB/ltx-models/gemma
  training_mode: lora
  # Load the SCD LoRA checkpoint as base (will be frozen)
  load_checkpoint: /home/johndpope/Documents/GitHub/sparse-causal-diffusion/outputs/scd_finetune_v1/checkpoints

acceleration:
  mixed_precision_mode: bf16
  quantization: int8-quanto
  load_text_encoder_in_8bit: false

hardware:
  devices:
    transformer: cuda:0
    vae_encoder: cuda:0
    vae_decoder: cuda:1
    text_encoder: cuda:0
    audio_vae: cuda:0
    vocoder: cuda:0

data:
  preprocessed_data_root: /media/2TB/isometric_i2v_training
  use_cached_final_embeddings: true
  final_embeddings_dir: conditions_final
  num_dataloader_workers: 2

lora:
  rank: 64           # EditCtrl LoRA rank (separate from SCD's rank 32)
  alpha: 64
  dropout: 0.0
  target_modules:
    - to_k
    - to_q
    - to_v
    - to_out.0

# HRR text enhancement config
hrr:
  enabled: true
  mode: token_aware
  num_channels: 16
  router_type: dot_product
  temperature: 1.0
  lr_multiplier: 5.0
  solo_warmup_steps: 50
  gate_init_bias: -2.0
  init_noise: 0.01
  entropy_reg_weight: 0.001
  gate_encourage_weight: 0.001

optimization:
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 0.00001  # Lower LR than SCD (1e-5 vs 1e-4) since base is trained
  max_grad_norm: 1.0
  optimizer_type: adamw
  weight_decay: 0.0001
  scheduler_type: cosine
  scheduler_params: {}
  steps: 3000
  enable_gradient_checkpointing: true

flow_matching:
  timestep_sampling_mode: shifted_logit_normal
  timestep_sampling_params: {}

training_strategy:
  name: editctrl_scd
  # SCD base params (inherited)
  encoder_layers: 32
  decoder_input_combine: token_concat
  clean_context_ratio: 0.0
  decoder_multi_batch: 1
  first_frame_conditioning_p: 0.0  # No first-frame conditioning for editing
  with_audio: false
  log_reconstructions: true
  reconstruction_log_interval: 100
  # EditCtrl params
  editctrl_phase: 1             # Phase 1: local context only
  local_num_blocks: 4           # 4 lightweight transformer blocks
  mask_dilation_latent: 2       # Dilate mask by 2 latent tokens
  global_context_num_tokens: 256
  mask_min_area: 0.05           # 5% minimum mask area
  mask_max_area: 0.6            # 60% maximum mask area
  freeze_base_lora: true        # Keep SCD LoRA frozen
  gradient_checkpointing_local: true
  # HRR integration
  use_hrr: true
  cross_caption_loss_weight: 0.1

checkpoints:
  interval: 500
  keep_last_n: 3
  precision: bfloat16

wandb:
  enabled: true
  project: editctrl-scd
  entity: null
  log_validation_videos: false
  tags:
    - editctrl
    - scd
    - hrr
    - phase1
    - local-context

validation:
  interval: null
  skip_initial_validation: true
  prompts: []
  seed: 42
  inference_steps: 20
  guidance_scale: 4.0
  negative_prompt: "worst quality, blurry, distorted, artifacts"
  frame_rate: 25.0
  video_dims: [512, 288, 33]
  videos_per_prompt: 1
  images: null
  reference_videos: null
  generate_audio: false
  include_reference_in_output: false
  stg_mode: stg_v
  stg_scale: 1.0
  stg_blocks:
    - 29
