# EditCtrl + SCD Phase 1: Local Context Only
#
# Trains LocalContextModule (4 lightweight transformer blocks) + EditCtrl LoRA (rank 64)
# on top of frozen SCD base. Synthetic rectangle/ellipse masks are generated on-the-fly.
#
# Phase 1 teaches the model to fill in masked regions using local source token context
# and text guidance. The zero-initialized gate_proj ensures stable training start.
#
# Prerequisites:
#   - Trained SCD LoRA checkpoint (from scd_finetune.yaml)
#   - Pre-encoded latents and cached embeddings
#
# Usage:
#   cd /home/johndpope/Documents/GitHub/ltx2-omnitransfer/packages/ltx-trainer
#   python -m ltx_trainer.train --config /home/johndpope/Documents/GitHub/sparse-causal-diffusion/configs/editctrl_scd_phase1.yaml

seed: 42
output_dir: /home/johndpope/Documents/GitHub/sparse-causal-diffusion/outputs/editctrl_scd_phase1

model:
  model_path: /media/2TB/ltx-models/ltx2/ltx-2-19b-dev.safetensors
  text_encoder_path: /media/2TB/ltx-models/gemma
  training_mode: lora
  # Load the SCD LoRA checkpoint as base (will be frozen)
  load_checkpoint: /home/johndpope/Documents/GitHub/sparse-causal-diffusion/outputs/scd_finetune_v1/checkpoints

acceleration:
  mixed_precision_mode: bf16
  quantization: int8-quanto
  load_text_encoder_in_8bit: false

hardware:
  devices:
    transformer: cuda:0
    vae_encoder: cuda:0
    vae_decoder: cuda:1
    text_encoder: cuda:0
    audio_vae: cuda:0
    vocoder: cuda:0

data:
  preprocessed_data_root: /media/2TB/isometric_i2v_training
  use_cached_final_embeddings: true
  final_embeddings_dir: conditions_final
  num_dataloader_workers: 2

lora:
  rank: 64           # EditCtrl LoRA rank (separate from SCD's rank 32)
  alpha: 64
  dropout: 0.0
  target_modules:
    - to_k
    - to_q
    - to_v
    - to_out.0

optimization:
  batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 0.00001  # Lower LR than SCD (1e-5 vs 1e-4) since base is trained
  max_grad_norm: 1.0
  optimizer_type: adamw
  weight_decay: 0.0001
  scheduler_type: cosine
  scheduler_params: {}
  steps: 3000
  enable_gradient_checkpointing: true

flow_matching:
  timestep_sampling_mode: shifted_logit_normal
  timestep_sampling_params: {}

training_strategy:
  name: editctrl_scd
  # SCD base params (inherited)
  encoder_layers: 32
  decoder_input_combine: token_concat
  clean_context_ratio: 0.0
  decoder_multi_batch: 1
  first_frame_conditioning_p: 0.0  # No first-frame conditioning for editing
  with_audio: false
  log_reconstructions: true
  reconstruction_log_interval: 100
  # EditCtrl params
  editctrl_phase: 1             # Phase 1: local context only
  local_num_blocks: 4           # 4 lightweight transformer blocks
  mask_dilation_latent: 2       # Dilate mask by 2 latent tokens
  global_context_num_tokens: 256
  mask_min_area: 0.05           # 5% minimum mask area
  mask_max_area: 0.6            # 60% maximum mask area
  freeze_base_lora: true        # Keep SCD LoRA frozen
  gradient_checkpointing_local: true

checkpoints:
  interval: 500
  keep_last_n: 3
  precision: bfloat16

wandb:
  enabled: true
  project: editctrl-scd
  entity: null
  log_validation_videos: false
  tags:
    - editctrl
    - scd
    - phase1
    - local-context

validation:
  interval: null
  skip_initial_validation: true
  prompts: []
  seed: 42
  inference_steps: 20
  guidance_scale: 4.0
  negative_prompt: "worst quality, blurry, distorted, artifacts"
  frame_rate: 25.0
  video_dims: [512, 288, 33]
  videos_per_prompt: 1
  images: null
  reference_videos: null
  generate_audio: false
  include_reference_in_output: false
  stg_mode: stg_v
  stg_scale: 1.0
  stg_blocks:
    - 29
